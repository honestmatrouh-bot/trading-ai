from pathlib import Path
import pandas as pd
import numpy as np

# =========================
# Ù…Ø³Ø§Ø±Ø§Øª Ø±Ø¦ÙŠØ³ÙŠØ©
# =========================
BASE_DIR = Path(__file__).resolve().parent
CASE_DIR = BASE_DIR / "CASE"
INTRADAY_DIR = BASE_DIR / "intraday"
TRANSACTION_DIR = BASE_DIR / "transaction"
MODELS_DIR = BASE_DIR / "models"
DATA_DIR = BASE_DIR / "data"

# Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø¹Ø§Ù…Ø© (Ù„Ø§ Ù†Ø¹Ø±Ø¶Ù‡Ø§ ÙˆØ³Ø· Ø§Ù„Ø£Ø³Ù‡Ù…)
INDEX_SYMBOLS = {
    "EGX30", "EGX70", "EGX100",
    "EGX100 EWI", "EGX70 EWI",
    "EGX30ETF", "EGX30TR",
    "SHARIAH", "EGX33 Shariah Index"
}


# =========================
# Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© Ù„ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
# =========================

def normalize_intraday_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© ÙˆØªÙˆØ­ÙŠØ¯ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡."""
    df = df.copy()

    rename_map = {
        "Ø§Ù„Ø±Ù…Ø²": "Symbol",
        "Ø§Ù„Ø¥Ø³Ù… Ø§Ù„Ù…Ø®ØªØµØ±": "S. Description",
        "Ø§Ù„Ø§Ø³Ù… Ø§Ù„Ù…Ø®ØªØµØ±": "S. Description",
        "Ø£Ø®Ø± Ø³Ø¹Ø±": "Last",
        "Ø¢Ø®Ø± Ø³Ø¹Ø±": "Last",
        "Ø§Ù„ØªØºÙŠØ± %": "% Change",
        "Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„": "Volume",
        "Ù‚ÙŠÙ…Ø© Ø§Ù„ØªØ¯Ø§ÙˆÙ„": "Turnover",
        "Ø­Ø¬Ù… Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ø¯Ø§Ø®Ù„Ø©": "Cash In Volume",
        "Ø­Ø¬Ù… Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ø®Ø§Ø±Ø¬Ø©": "Cash Out Volume",
        "Ø§Ù„ØµÙÙ‚Ø§Øª": "Trades",
        "Ù…Ø®Ø·Ø· Ø§Ù„Ø³ÙŠÙˆÙ„Ø© %": "Range",  # Ø¨Ø¹Ø¶ Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ ØªØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ù„Ù†Ø³Ø¨Ø© Ù…Ø¯Ù‰ Ø§Ù„Ø­Ø±ÙƒØ©
        "(R1) Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© 1": "Resistance 1 (R1)",
        "(R2) Ø§Ù„Ù…Ù‚Ø§ÙˆÙ…Ø© 2": "Resistance 2 (R2)",
        "(S1) Ø§Ù„Ø¯Ø¹Ù… 1": "Support 1 (S1)",
        "(S2) Ø§Ù„Ø¯Ø¹Ù… 2": "Support 2 (S2)",
        "Ø§Ù„Ø£Ø¯Ù†Ù‰": "Low",
        "Ø£Ø¹Ù„Ù‰": "High",
        "ÙØªØ­": "Open",
        "Ø¥ØºÙ„Ø§Ù‚": "Close",
        "Ø§Ù„Ø£Ø¯Ù†Ù‰ Ø®Ù„Ø§Ù„ 52 Ø£Ø³Ø¨ÙˆØ¹": "52 week Low",
        "Ø§Ù„Ø£Ø¹Ù„Ù‰ Ø®Ù„Ø§Ù„ 52 Ø£Ø³Ø¨ÙˆØ¹": "52 week High",
        "Ù†Ø³Ø¨Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¹Ø±Ø¶": "Bid Offer Ratio",
        "Ø§Ù„Ø·Ù„Ø¨": "Bid",
        "Ø§Ù„Ø¹Ø±Ø¶": "Offer",
        "ÙƒÙ…ÙŠØ© Ø§Ù„Ø·Ù„Ø¨": "Bid Qty.",
        "ÙƒÙ…ÙŠØ© Ø§Ù„Ø¹Ø±Ø¶": "Offer Qty.",
        "Ø§Ù„Ù‚Ø·Ø§Ø¹": "Sector",
        "Ù…Ø¶Ø§Ø¹Ù Ø±Ø¨Ø­ÙŠØ© Ø§Ù„Ø³Ù‡Ù…": "P-E Ratio",
        "Ù…Ø¶Ø§Ø¹Ù Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¯ÙØªØ±ÙŠØ©": "P-B Ratio",
        "Ø±Ø¨Ø­ÙŠØ© Ø§Ù„Ø³Ù‡Ù…": "Earning Per Share",
        "% Ø§Ù„Ù…Ø¯Ù‰": "Range",
        "ØµÙÙ‚Ø§Øª Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ø¯Ø§Ø®Ù„Ø©": "Cash In Trades",
        "ØµÙÙ‚Ø§Øª Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ø®Ø§Ø±Ø¬Ø©": "Cash Out Trades",
        "Ù‚ÙŠÙ…Ø© Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ø®Ø§Ø±Ø¬Ø©": "Cash Out Turnover",
        "Ù‚ÙŠÙ…Ø© Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ø¯Ø§Ø®Ù„Ø©": "Cash in Turnover",  # Ù†Ø³ØªØ®Ø¯Ù… Ù†ÙØ³ Ø§Ù„Ø§Ø³Ù… ÙÙ‰ app
        "Ù…Ø¤Ø´Ø± Ø§Ù„Ø³ÙŠÙˆÙ„Ø© Ø§Ù„Ù†Ù‚Ø¯ÙŠØ©": "Cash Flow Index",
        "Ù†Ù‚Ø·Ø© Ø§Ù„Ø¥Ø±ØªÙƒØ§Ø²": "Pivot Point",
        "Ø±Ø³Ù…Ù„Ø© Ø§Ù„Ø³ÙˆÙ‚ Ø¨Ø§Ù„Ø¢Ù„Ø§Ù": "Mkt. Cap./1000",
        "ØªØºÙŠØ± Ù…ØªÙˆØ³Ø· Ø§Ù„Ø³Ø¹Ø± Ø§Ù„Ù…Ø±Ø¬Ø­ %": "VWAP Change",
        "Ø¥Ù‚ÙØ§Ù„ Ø³Ø§Ø¨Ù‚": "Prev. Closed",
        "Ù†Ø³Ø¨Ø© Ø§Ù„Ø³ÙŠÙˆÙ„Ø©": "Cash Map % Value",
        "S. Description": "S. Description",  # Ù„Ùˆ Ù‡Ù‰ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¥Ù†Ø¬Ù„ÙŠØ²Ù‰
    }
    df = df.rename(columns=rename_map)

    # ğŸ”§ Ø£Ù‡Ù… ØªØ¹Ø¯ÙŠÙ„: Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ©
    # Ù„Ùˆ ÙƒØ§Ù† Ø¹Ù†Ø¯Ùƒ "Ø£Ø®Ø± Ø³Ø¹Ø±" Ùˆ"Ø¢Ø®Ø± Ø³Ø¹Ø±" Ø§Ù„Ø§ØªÙ†ÙŠÙ† Ø¨Ù‚ÙˆØ§ Last -> Ù†Ø­Ø§ÙØ¸ Ø¹Ù„Ù‰ Ø£ÙˆÙ„ ÙˆØ§Ø­Ø¯
    df = df.loc[:, ~df.columns.duplicated()]

    return df


def normalize_transactions_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ØªÙˆØ­ÙŠØ¯ Ø£Ø¹Ù…Ø¯Ø© Ù…Ù„Ù Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©."""
    df = df.copy()

    rename_map = {
        "Ø§Ø³Ù… Ø§Ù„Ø³Ù‡Ù…": "Description",
        "Ø§Ù„Ø¥Ø³Ù… Ø§Ù„Ù…Ø®ØªØµØ±": "Description",
        "Ø§Ù„Ø§Ø³Ù…": "Description",
        "Ø§Ù„Ø±Ù…Ø²": "Symbol",
        "Ø§Ù„Ø³Ø¹Ø±": "Price",
        "Ø§Ù„Ù†ÙˆØ¹": "Side",     # B / S Ø£Ùˆ Buy / Sell
        "Ø§Ù„ØªØºÙŠØ± %": "% Change",
        "Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„": "Volume",
        "Ù‚ÙŠÙ…Ø© Ø§Ù„ØªØ¯Ø§ÙˆÙ„": "Turnover",
        "Ù…ÙØ¹Ø±Ù Ø§Ù„ØªØ³Ù„Ø³Ù„": "Sequence ID",
        "Ø§Ù„ÙˆÙ‚Øª": "Time",
        "Tick": "Tick",
        "Ø¥ØªØ¬Ø§Ù‡": "Direction",   # 2 / 1 / -2 ... Ø§Ù„Ø®
        "Ø§ØªØ¬Ø§Ù‡": "Direction",
    }
    df = df.rename(columns=rename_map)
    df = df.loc[:, ~df.columns.duplicated()]

    # ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø©
    for col in ["Symbol", "Side", "Volume", "Turnover"]:
        if col not in df.columns:
            df[col] = np.nan
    return df


def normalize_case_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ØªÙˆØ­ÙŠØ¯ Ø£Ø¹Ù…Ø¯Ø© Ù…Ù„ÙØ§Øª CASE Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©."""
    df = df.copy()

    rename_map = {
        "Ø§Ù„ØªØ§Ø±ÙŠØ®": "Date",
        "ÙØªØ­": "Open",
        "Ø£Ø¹Ù„Ù‰": "High",
        "Ø§Ù„Ø£Ø¯Ù†Ù‰": "Low",
        "Ù…ØºÙ„Ù‚": "Closed",
        "Ø¥Ù‚ÙØ§Ù„ Ø³Ø§Ø¨Ù‚": "Prev. Closed",
        "Ø§Ù„ØªØºÙŠØ± %": "%Chg",
        "Ø§Ù„ØªØºÙŠØ±": "Chg.",
        "Ù‚ÙŠÙ…Ø© Ø§Ù„ØªØ¯Ø§ÙˆÙ„": "Turnover",
        "Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„": "Volume",
    }
    df = df.rename(columns=rename_map)
    df = df.loc[:, ~df.columns.duplicated()]

    if "Date" in df.columns:
        df["Date"] = pd.to_datetime(df["Date"], dayfirst=True, errors="coerce")
    return df


# =========================
# ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
# =========================

def load_intraday(path: Path) -> pd.DataFrame:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù intraday (XLSX) Ù…Ø¹ ØªØ­Ø¯ÙŠØ¯ openpyxl ÙƒÙ€ engine."""
    df = pd.read_excel(path, engine="openpyxl")
    df = normalize_intraday_columns(df)

    core = ["Symbol", "S. Description", "Last", "% Change", "Open", "High", "Low", "Volume"]
    missing = [c for c in core if c not in df.columns]
    if missing:
        print("âš ï¸ Intraday missing core columns:", missing)

    # Ù†Ø­Ø§ÙˆÙ„ ØªØ­ÙˆÙŠÙ„ Ø¨Ø¹Ø¶ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ù‡Ù…Ø© Ù„Ø£Ø±Ù‚Ø§Ù…
    for c in ["Last", "% Change", "Open", "High", "Low", "Close", "Prev. Closed",
              "Range", "Volume", "Turnover"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙŠØ§Øª Pivot Ù„Ùˆ Ù†Ø§Ù‚ØµØ© / ÙÙŠÙ‡Ø§ NaN
    df = add_pivot_levels(df)

    return df


def load_transactions(path: Path) -> pd.DataFrame:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø© Ù…Ø¹ ØªØ¬Ø±Ø¨Ø© Ø£ÙƒØ«Ø± Ù…Ù† ØªØ±Ù…ÙŠØ²."""
    encodings_to_try = ["utf-8-sig", "utf-16", "cp1256", "cp1252"]

    df = None
    for enc in encodings_to_try:
        try:
            df = pd.read_csv(path, encoding=enc)
            df = normalize_transactions_columns(df)
            print(f"Loaded transactions using encoding: {enc}")
            break
        except Exception:
            df = None

    if df is None:
        df = pd.read_csv(path, encoding="latin1", errors="replace")
        df = normalize_transactions_columns(df)
        print("âš ï¸ Loaded transactions with fallback encoding (latin1 with replacement).")

    # ØªØ­ÙˆÙŠÙ„ Ø£Ø±Ù‚Ø§Ù…
    for c in ["Price", "% Change", "Volume", "Turnover"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    return df


def load_case(symbol: str) -> pd.DataFrame:
    """Ù‚Ø±Ø§Ø¡Ø© Ù…Ù„Ù CASE Ø§Ù„ØªØ§Ø±ÙŠØ®Ù‰ Ù„Ø³Ù‡Ù… Ù…Ø¹ÙŠÙ† Ù…Ø¹ Ø¯Ø¹Ù… ØªØ±Ù…ÙŠØ²Ø§Øª Ø¹Ø±Ø¨ÙŠØ©."""
    path = CASE_DIR / f"{symbol}.csv"

    encodings_to_try = ["utf-8-sig", "utf-16", "cp1256", "cp1252"]
    df = None
    for enc in encodings_to_try:
        try:
            df = pd.read_csv(path, encoding=enc)
            df = normalize_case_columns(df)
            print(f"Loaded CASE for {symbol} using encoding: {enc}")
            break
        except Exception:
            df = None

    if df is None:
        df = pd.read_csv(path, encoding="latin1", errors="replace")
        df = normalize_case_columns(df)
        print(f"âš ï¸ Loaded CASE for {symbol} with fallback encoding (latin1 with replacement).")

    # ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¥ØºÙ„Ø§Ù‚ Ø£Ø±Ù‚Ø§Ù…
    for c in ["Open", "High", "Low", "Closed", "Prev. Closed", "Turnover", "Volume"]:
        if c in df.columns:
            df[c] = pd.to_numeric(df[c], errors="coerce")

    return df


# =========================
# ØªØ­Ù„ÙŠÙ„Ø§Øª Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©
# =========================

def aggregate_transactions(df_tx: pd.DataFrame) -> pd.DataFrame:
    """ØªÙ„Ø®ÙŠØµ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø© Ù„ÙƒÙ„ Ø³Ù‡Ù… (Ø­Ø¬Ù…ØŒ Ø³ÙŠÙˆÙ„Ø©ØŒ ØªØ¬Ù…ÙŠØ¹/ØªØµØ±ÙŠÙ)."""
    if df_tx is None or df_tx.empty:
        return pd.DataFrame(columns=["Symbol", "total_volume", "total_turnover",
                                     "buy_volume", "sell_volume", "buy_ratio",
                                     "behavior"])

    df = df_tx.copy()
    df["Volume"] = pd.to_numeric(df["Volume"], errors="coerce").fillna(0)
    df["Turnover"] = pd.to_numeric(df["Turnover"], errors="coerce").fillna(0)

    # ØªØ­Ø¯ÙŠØ¯ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø´Ø±Ø§Ø¡ ÙˆØ§Ù„Ø¨ÙŠØ¹:
    side = df["Side"].astype(str).str.upper()

    buy_mask = side.isin(["B", "BUY"])
    sell_mask = side.isin(["S", "SELL"])

    # Ù„Ùˆ Ù…ÙÙŠØ´ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙÙ‰ Side Ù†Ø­Ø§ÙˆÙ„ Ù†Ø³ØªÙÙŠØ¯ Ù…Ù† Direction (2 / -2 / 1...)
    if (not buy_mask.any()) and ("Direction" in df.columns):
        dir_col = pd.to_numeric(df["Direction"], errors="coerce")
        buy_mask = dir_col.gt(0)
        sell_mask = dir_col.lt(0)

    grouped = df.groupby("Symbol")

    total_volume = grouped["Volume"].sum()
    total_turnover = grouped["Turnover"].sum()
    buy_volume = df[buy_mask].groupby("Symbol")["Volume"].sum()
    sell_volume = df[sell_mask].groupby("Symbol")["Volume"].sum()

    agg = pd.DataFrame({
        "total_volume": total_volume,
        "total_turnover": total_turnover,
        "buy_volume": buy_volume,
        "sell_volume": sell_volume,
    }).fillna(0)

    agg["buy_ratio"] = agg.apply(
        lambda r: r["buy_volume"] / r["total_volume"] if r["total_volume"] > 0 else np.nan,
        axis=1
    )

    def classify_behavior(row):
        if np.isnan(row["buy_ratio"]):
            return "Normal"
        if row["buy_ratio"] > 0.6:
            return "Accumulation"
        if row["buy_ratio"] < 0.4:
            return "Distribution"
        return "Normal"

    agg["behavior"] = agg.apply(classify_behavior, axis=1)

    agg.reset_index(inplace=True)
    return agg


# =========================
# Ù…Ø¤Ø´Ø±Ø§Øª ÙÙ†ÙŠØ© Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ù† CASE
# =========================

def compute_basic_technicals(df_case: pd.DataFrame) -> pd.DataFrame:
    """
    Ø­Ø³Ø§Ø¨ Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„ÙŠÙˆÙ…ØŒ MA20ØŒ MA50ØŒ Ù…ØªÙˆØ³Ø· Ø­Ø¬Ù… 20 ÙŠÙˆÙ….
    ÙŠØ±Ø¬Ø¹ ØµÙØ§Ù‹ ÙˆØ§Ø­Ø¯Ø§Ù‹ ÙŠÙ…Ø«Ù„ Ø¢Ø®Ø± ÙŠÙˆÙ….
    """
    df = df_case.sort_values("Date").copy()

    if "Closed" in df.columns:
        close = pd.to_numeric(df["Closed"], errors="coerce")
    elif "Close" in df.columns:
        close = pd.to_numeric(df["Close"], errors="coerce")
    else:
        raise ValueError("Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¹Ù…ÙˆØ¯ Closed/Close ÙÙ‰ Ø¨ÙŠØ§Ù†Ø§Øª CASE.")

    df["Close_val"] = close
    df["MA20"] = df["Close_val"].rolling(20).mean()
    df["MA50"] = df["Close_val"].rolling(50).mean()

    if "Volume" in df.columns:
        df["Vol20"] = pd.to_numeric(df["Volume"], errors="coerce").rolling(20).mean()
    else:
        df["Vol20"] = np.nan

    last_row = df.iloc[[-1]][["Close_val", "MA20", "MA50", "Vol20"]]
    last_row.rename(columns={"Close_val": "Close"}, inplace=True)
    return last_row


# =========================
# Ø­Ø³Ø§Ø¨ Pivot / R1 / R2 / S1 / S2
# =========================

def add_pivot_levels(df_intraday: pd.DataFrame) -> pd.DataFrame:
    """
    Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©:
      Pivot Point, Resistance 1 (R1), Resistance 2 (R2),
      Support 1 (S1), Support 2 (S2)
    Ù„Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø© Ø£Ùˆ Ù…Ù„ÙŠØ§Ù†Ø© NaN Ù†Ø­Ø³Ø¨Ù‡Ø§ Ù…Ù†:
      P = (High + Low + Prev. Closed) / 3
    """
    df = df_intraday.copy()

    # Ù†ØªØ£ÙƒØ¯ Ø£Ù† Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ù…ÙˆØ¬ÙˆØ¯Ø©
    if "Pivot Point" not in df.columns:
        df["Pivot Point"] = np.nan
    if "Resistance 1 (R1)" not in df.columns:
        df["Resistance 1 (R1)"] = np.nan
    if "Resistance 2 (R2)" not in df.columns:
        df["Resistance 2 (R2)"] = np.nan
    if "Support 1 (S1)" not in df.columns:
        df["Support 1 (S1)"] = np.nan
    if "Support 2 (S2)" not in df.columns:
        df["Support 2 (S2)"] = np.nan

    high = pd.to_numeric(df.get("High"), errors="coerce")
    low = pd.to_numeric(df.get("Low"), errors="coerce")

    # Ù†Ø³ØªØ®Ø¯Ù… Ø¥Ù‚ÙØ§Ù„ Ø³Ø§Ø¨Ù‚ ÙƒÙ€ Close Ù„Ù„Ø­Ø³Ø§Ø¨ØŒ Ù„Ùˆ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø³ØªØ®Ø¯Ù… Last
    if "Prev. Closed" in df.columns:
        close_for_pivot = pd.to_numeric(df["Prev. Closed"], errors="coerce")
    else:
        close_for_pivot = pd.to_numeric(df.get("Last"), errors="coerce")

    # pivot / R / S
    mask_valid = (~high.isna()) & (~low.isna()) & (~close_for_pivot.isna())

    # Ù†Ø­Ø³Ø¨ ÙÙ‚Ø· Ù„Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ù„Ù‰ ÙÙŠÙ‡Ø§ NaN Ø£Ùˆ Ø§Ù„Ù‚ÙŠÙ… ÙƒÙ„Ù‡Ø§ ØµÙØ±
    need_calc = mask_valid & (
        df["Pivot Point"].isna()
        & df["Resistance 1 (R1)"].isna()
        & df["Resistance 2 (R2)"].isna()
        & df["Support 1 (S1)"].isna()
        & df["Support 2 (S2)"].isna()
    )

    P = (high + low + close_for_pivot) / 3.0
    R1 = 2 * P - low
    S1 = 2 * P - high
    R2 = P + (high - low)
    S2 = P - (high - low)

    df.loc[need_calc, "Pivot Point"] = P[need_calc]
    df.loc[need_calc, "Resistance 1 (R1)"] = R1[need_calc]
    df.loc[need_calc, "Resistance 2 (R2)"] = R2[need_calc]
    df.loc[need_calc, "Support 1 (S1)"] = S1[need_calc]
    df.loc[need_calc, "Support 2 (S2)"] = S2[need_calc]

    return df


# =========================
# Ø¨Ù†Ø§Ø¡ Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
# =========================

def build_signals_for_day(intraday_path: Path, tx_path: Path) -> pd.DataFrame:
    """Ø¯Ù…Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª intraday Ù…Ø¹ Ù…Ù„Ø®Øµ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø© Ù„Ø¥Ù†ØªØ§Ø¬ Ø¬Ø¯ÙˆÙ„ Ø¥Ø´Ø§Ø±Ø§Øª."""
    df_intraday = load_intraday(intraday_path)
    df_tx = load_transactions(tx_path)
    agg_tx = aggregate_transactions(df_tx)

    df = df_intraday.merge(agg_tx, on="Symbol", how="left")

    # Ø³ÙŠÙˆÙ„Ø© Ø¯Ø®ÙˆÙ„/Ø®Ø±ÙˆØ¬ Ù„Ùˆ Ù…Ø´ Ù…ÙˆØ¬ÙˆØ¯Ø©
    if "Cash in Turnover" not in df.columns:
        df["Cash in Turnover"] = df["total_turnover"].fillna(0)
    if "Cash Out Turnover" not in df.columns:
        df["Cash Out Turnover"] = 0

    return df


# =========================
# "Ù†Ù…ÙˆØ°Ø¬" Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹Ù‰ Ù…Ø¨Ø¯Ø¦Ù‰ Rule-based
# =========================

def apply_ai_score(signals: pd.DataFrame) -> pd.DataFrame:
    """
    Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ù†Ø¬Ø§Ø­ Ø§Ù„ÙØ±ØµØ© (AI_Prob) Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚ÙˆØ§Ø¹Ø¯ Ø¨Ø³ÙŠØ·Ø©:
    - Ø§ØªØ¬Ø§Ù‡ Ø­Ø±ÙƒØ© Ø§Ù„Ø³Ø¹Ø± (% Change)
    - buy_ratio (Ù…Ù† Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©)
    - Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¬Ù„Ø³Ø© (Accumulation/Distribution)
    """
    df = signals.copy()

    if "AI_Prob" in df.columns:
        return df

    df["AI_Prob"] = 0.5

    # ØªØ£Ø«ÙŠØ± Ø§Ù„ØªØºÙŠØ± Ø§Ù„ÙŠÙˆÙ…Ù‰
    if "% Change" in df.columns:
        change = pd.to_numeric(df["% Change"], errors="coerce").fillna(0)
        df["AI_Prob"] += np.where(change > 0, 0.07, np.where(change < 0, -0.07, 0))

    # ØªØ£Ø«ÙŠØ± buy_ratio
    if "buy_ratio" in df.columns:
        br = pd.to_numeric(df["buy_ratio"], errors="coerce")
        df["AI_Prob"] += 0.3 * (br - 0.5).fillna(0)

    # Ø³Ù„ÙˆÙƒ Ø§Ù„Ø¬Ù„Ø³Ø©
    if "behavior" in df.columns:
        df["AI_Prob"] += df["behavior"].map({
            "Accumulation": 0.08,
            "Distribution": -0.08
        }).fillna(0)

    # Ù‚Øµ Ø§Ù„Ù‚ÙŠÙ… Ø¨ÙŠÙ† 0.05 Ùˆ 0.95
    df["AI_Prob"] = df["AI_Prob"].clip(0.05, 0.95)

    return df


# =========================
# S/R Breakouts helper
# =========================

def find_sr_breakouts(df_intraday: pd.DataFrame):
    """
    ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø³Ù‡Ù… Ø§Ù„ØªÙ‰:
    - Ø£ØºÙ„Ù‚Øª ÙÙˆÙ‚ R1 Ø£Ùˆ R2
    - Ø£ØºÙ„Ù‚Øª ØªØ­Øª S1 Ø£Ùˆ S2
    Ù†Ø¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Last ÙƒØ³Ø¹Ø± Ø§Ù„Ø¬Ù„Ø³Ø©.
    """
    df = add_pivot_levels(df_intraday)

    price = pd.to_numeric(df.get("Last"), errors="coerce")
    r1 = pd.to_numeric(df.get("Resistance 1 (R1)"), errors="coerce")
    r2 = pd.to_numeric(df.get("Resistance 2 (R2)"), errors="coerce")
    s1 = pd.to_numeric(df.get("Support 1 (S1)"), errors="coerce")
    s2 = pd.to_numeric(df.get("Support 2 (S2)"), errors="coerce")

    cols_basic = ["Symbol", "S. Description", "Last", "% Change",
                  "Volume", "Resistance 1 (R1)", "Resistance 2 (R2)",
                  "Support 1 (S1)", "Support 2 (S2)"]

    r1_break = df[(price >= r1) & r1.notna()][cols_basic]
    r2_break = df[(price >= r2) & r2.notna()][cols_basic]
    s1_break = df[(price <= s1) & s1.notna()][cols_basic]
    s2_break = df[(price <= s2) & s2.notna()][cols_basic]

    return {
        "R1_break": r1_break.sort_values("% Change", ascending=False),
        "R2_break": r2_break.sort_values("% Change", ascending=False),
        "S1_break": s1_break.sort_values("% Change", ascending=True),
        "S2_break": s2_break.sort_values("% Change", ascending=True),
    }


# =========================
# T+0 / T+1 candidates
# =========================

def build_t0_t1_candidates(signals: pd.DataFrame, top_n: int = 5) -> pd.DataFrame:
    """
    Ø§Ø®ØªÙŠØ§Ø± Ø£Ø³Ù‡Ù… Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ø¶Ø§Ø±Ø¨Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø© T+0 / T+1:
    - ØªØ°Ø¨Ø°Ø¨ (Range) Ø¹Ø§Ù„Ù‰
    - Ø­Ø¬Ù… ØªØ¯Ø§ÙˆÙ„ Ø¬ÙŠØ¯
    - Ø³ÙŠÙˆÙ„Ø© Ù…Ø±ÙƒØ²Ø© (buy_ratio Ù‚Ø±ÙŠØ¨ Ù…Ù† 0.5 â€“ 0.7)
    - AI_Prob Ø¬ÙŠØ¯
    """
    df = signals.copy()

    # Range: Ù„Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ØŒ Ù†Ø­Ø³Ø¨Ù‡ ØªÙ‚Ø±ÙŠØ¨ÙŠØ§ Ù…Ù† (High-Low)/Last
    if "Range" in df.columns:
        rng = pd.to_numeric(df["Range"], errors="coerce")
    else:
        high = pd.to_numeric(df.get("High"), errors="coerce")
        low = pd.to_numeric(df.get("Low"), errors="coerce")
        last = pd.to_numeric(df.get("Last"), errors="coerce")
        rng = (high - low) / last.replace(0, np.nan) * 100
    df["Range_calc"] = rng

    vol = pd.to_numeric(df.get("Volume"), errors="coerce").fillna(0)
    ai_prob = pd.to_numeric(df.get("AI_Prob"), errors="coerce").fillna(0.5)
    buy_ratio = pd.to_numeric(df.get("buy_ratio"), errors="coerce")

    # ÙÙ„ØªØ±Ø© Ù…Ø¨Ø¯Ø¦ÙŠØ©
    mask = (vol >= vol.quantile(0.5)) & (df["Range_calc"] >= df["Range_calc"].quantile(0.5))
    cand = df[mask].copy()

    # Ø¯Ø±Ø¬Ø© T0/T1
    cand["T0T1_Score"] = (
        100 * ai_prob +
        3 * cand["Range_calc"].fillna(0) +
        50 * (buy_ratio.fillna(0.5) - 0.5)
    )

    cols = ["Symbol", "S. Description", "Last", "% Change", "Volume",
            "Range_calc", "buy_ratio", "behavior", "AI_Prob", "T0T1_Score"]
    cols = [c for c in cols if c in cand.columns]

    cand = cand.sort_values("T0T1_Score", ascending=False).head(top_n)
    return cand[cols]


# =========================
# Ø¹Ù„Ø§Ù‚Ø§Øª Ø§Ù„Ø£Ø³Ù‡Ù… Ù…Ù† CASE (Correlation)
# =========================

def build_stock_relationships(
    intraday_df: pd.DataFrame,
    min_days: int = 60,
    min_abs_corr: float = 0.7,
    top_n: int = 40
) -> pd.DataFrame:
    """
    Ø­Ø³Ø§Ø¨ Ø¹Ù„Ø§Ù‚Ø§Øª (Correlation) Ø¨ÙŠÙ† Ø¹ÙˆØ§Ø¦Ø¯ Ø§Ù„Ø£Ø³Ù‡Ù… Ø§Ù„Ù…Ø¹Ø±ÙˆØ¶Ø© ÙÙ‰ Ø¬Ù„Ø³Ø© Ø§Ù„ÙŠÙˆÙ…
    Ø§Ø¹ØªÙ…Ø§Ø¯Ø§Ù‹ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª CASE (Ø¥ØºÙ„Ø§Ù‚ ÙŠÙˆÙ…Ù‰).
    - Ù†Ø®ØªØ§Ø± top_n Ø£Ø³Ù‡Ù… Ù…Ù† Ø­ÙŠØ« Ø­Ø¬Ù… Ø§Ù„ØªØ¯Ø§ÙˆÙ„ ÙÙ‰ intraday
    - Ù†Ø­Ø³Ø¨ Ø§Ù„Ø¹Ø§Ø¦Ø¯ Ø§Ù„ÙŠÙˆÙ…Ù‰ (Ù†Ø³Ø¨Ø© Ø§Ù„ØªØºÙŠØ±) Ù„ÙƒÙ„ Ø³Ù‡Ù…
    - Ù†Ø¨Ù†Ù‰ Ù…ØµÙÙˆÙØ© Ø§Ø±ØªØ¨Ø§Ø·
    - Ù†Ø®Ø±Ø¬ Ø§Ù„Ø£Ø²ÙˆØ§Ø¬ Ø°Ø§Øª |corr| >= min_abs_corr
    """
    if intraday_df is None or intraday_df.empty:
        return pd.DataFrame(columns=["Symbol_A", "Symbol_B", "Corr", "Relation"])

    df_intra = intraday_df.copy()
    if "Volume" in df_intra.columns:
        df_intra["Volume"] = pd.to_numeric(df_intra["Volume"], errors="coerce").fillna(0)
        df_intra = df_intra.sort_values("Volume", ascending=False)
    symbols = df_intra["Symbol"].astype(str).dropna().unique().tolist()
    symbols = symbols[:top_n]

    returns_dict = {}
    for sym in symbols:
        try:
            df_case = load_case(sym)
            if df_case.empty:
                continue
            df_case = df_case.sort_values("Date")
            if "Closed" in df_case.columns:
                close = pd.to_numeric(df_case["Closed"], errors="coerce")
            elif "Close" in df_case.columns:
                close = pd.to_numeric(df_case["Close"], errors="coerce")
            else:
                continue
            ret = close.pct_change()
            ret.name = sym
            returns_dict[sym] = ret
        except FileNotFoundError:
            continue
        except Exception:
            continue

    if not returns_dict:
        return pd.DataFrame(columns=["Symbol_A", "Symbol_B", "Corr", "Relation"])

    # Ø¯Ù…Ø¬ ÙƒÙ„ Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ù…Ø´ØªØ±Ùƒ
    returns_df = pd.concat(returns_dict.values(), axis=1, join="inner").dropna(how="all")
    if len(returns_df) < min_days:
        # Ù„Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ù„ÙŠÙ„Ø© Ø¬Ø¯Ù‹Ø§
        return pd.DataFrame(columns=["Symbol_A", "Symbol_B", "Corr", "Relation"])

    corr_mat = returns_df.corr()

    rows = []
    syms = corr_mat.columns.tolist()
    for i in range(len(syms)):
        for j in range(i + 1, len(syms)):
            a = syms[i]
            b = syms[j]
            c = corr_mat.loc[a, b]
            if pd.isna(c):
                continue
            if abs(c) >= min_abs_corr:
                rel = "Positive" if c > 0 else "Negative"
                rows.append({"Symbol_A": a, "Symbol_B": b, "Corr": float(c), "Relation": rel})

    if not rows:
        return pd.DataFrame(columns=["Symbol_A", "Symbol_B", "Corr", "Relation"])

    rel_df = pd.DataFrame(rows)
    rel_df["AbsCorr"] = rel_df["Corr"].abs()
    rel_df = rel_df.sort_values("AbsCorr", ascending=False)

    # Ù†Ø¹ÙŠØ¯ ÙÙ‚Ø· Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø¹Ø±Ø¶
    rel_df = rel_df[["Symbol_A", "Symbol_B", "Corr", "Relation"]]
    return rel_df


# =========================
# Group Picks helper
# =========================

def filter_group_picks(signals: pd.DataFrame, symbols_list) -> pd.DataFrame:
    """
    ÙÙ„ØªØ±Ø© Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¥Ø´Ø§Ø±Ø§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø±Ù…ÙˆØ² Ù…Ù† Ø¬Ø±ÙˆØ¨Ø§Øª Ø§Ù„ØªÙˆØµÙŠØ§ØªØŒ
    ÙˆØ¥Ø±Ø¬Ø§Ø¹Ù‡Ø§ Ù…Ø±ØªØ¨Ø© ØªÙ†Ø§Ø²Ù„ÙŠØ§Ù‹ Ø­Ø³Ø¨ AI_Prob.
    """
    df = signals.copy()
    df["Symbol"] = df["Symbol"].astype(str)
    mask = df["Symbol"].isin(symbols_list)
    df = df[mask].copy()

    if "AI_Prob" not in df.columns:
        df["AI_Prob"] = 0.5

    cols_show = ["Symbol", "S. Description", "% Change", "Volume",
                 "buy_ratio", "behavior", "AI_Prob"]
    cols_show = [c for c in cols_show if c in df.columns]

    df = df.sort_values("AI_Prob", ascending=False)
    return df[cols_show]
